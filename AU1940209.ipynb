{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMaiHPUF_s7w"
      },
      "source": [
        "# Music Recommender System using Apache Spark and Python\n",
        "Urvish Makwana\n",
        "AU1940209\n",
        "\n",
        "## Description\n",
        "\n",
        "For final project, I have created a recommender system that will recommend new musical artists to a user based on their listening history. Suggesting different songs or musical artists to a user is important to many music streaming services, such as Pandora and Spotify. In addition, this type of recommender system could also be used as a means of suggesting TV shows or movies to a user.\n",
        "\n",
        "Here, I have used Spark and the collaborative filtering technique.\n",
        "\n",
        "\n",
        "## Datasets\n",
        "\n",
        "http://www-etud.iro.umontreal.ca/~bergstrj/audioscrobbler_data.html\n",
        "\n",
        "The data is modified from the original data files so that the code will run in a reasonable time on a single machine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VB0u72p2-7yq"
      },
      "source": [
        "## Necessary Package Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "baJXldPl-7yr"
      },
      "outputs": [],
      "source": [
        "from pyspark.mllib.recommendation import *\n",
        "import random\n",
        "from operator import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCLlpnM-ujvQ"
      },
      "source": [
        "## Loading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4U5FwfN-7yu"
      },
      "outputs": [],
      "source": [
        "#Loading data into RDD\n",
        "artistData = sc.textFile(\"artist_data_small.txt\")\n",
        "artistAlias = sc.textFile(\"artist_alias_small.txt\")\n",
        "userArtistData = sc.textFile(\"user_artist_data_small.txt\")\n",
        "\n",
        "alias_data = artistAlias.collect()\n",
        "user_data = userArtistData.collect()\n",
        "artist_canonical_dict = {}\n",
        "user_list = []\n",
        "\n",
        "for line in alias_data:\n",
        "    artist_record = line.split(\"\\t\")\n",
        "    artist_canonical_dict[artist_record[0]] = artist_record[1]\n",
        "\n",
        "#Function to get canonical artist names\n",
        "def canonicalArtistID(line):\n",
        "    line = line.split(\" \")\n",
        "    \n",
        "    if line[1] in artist_canonical_dict:\n",
        "        return (int(line[0]),int(artist_canonical_dict[line[1]]),int(line[2]))\n",
        "    else:\n",
        "        return (int(line[0]),int(line[1]),int(line[2]))\n",
        "    \n",
        "#Getting canonical artist names        \n",
        "userArtistData = userArtistData.map(canonicalArtistID)\n",
        "\n",
        "#Creating allArtists dataset to be used later during model evaluation process\n",
        "allArtists = userArtistData.map(lambda x:(x[1])).collect()\n",
        "allArtists = list(set(allArtists))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5yMIaIjujvR"
      },
      "source": [
        "## Data Exploration\n",
        "users with the highest number of total play counts.*italicized text*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJJlKo3M-7yv",
        "outputId": "2295f3cb-077c-4f25-e8ec-8f2bf41aae92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User 1059637 has a total play count of 674412 and a mean play count of 1878\n",
            "User 2064012 has a total play count of 548427 and a mean play count of 9455\n",
            "User 2069337 has a total play count of 393515 and a mean play count of 1519\n"
          ]
        }
      ],
      "source": [
        "artist_data = artistAlias.collect()\n",
        "    \n",
        "user_play_count = {}\n",
        "user_count_number = {}\n",
        "\n",
        "for line in user_data:\n",
        "     user_record = line.split()\n",
        "     if user_record[0] in user_play_count:\n",
        "         user_play_count[str(user_record[0])] = user_play_count[user_record[0]] + int(user_record[2])\n",
        "         user_count_number[str(user_record[0])] = user_count_number[user_record[0]] + 1\n",
        "     else:\n",
        "         user_play_count[str(user_record[0])] = int(user_record[2])\n",
        "         user_count_number[str(user_record[0])] = 1\n",
        "top = 0\n",
        "maximum = 2\n",
        "\n",
        "for word, count in sorted(user_play_count.iteritems(), key=lambda (k,v): (v,k), reverse = True):\n",
        "     if top > maximum:\n",
        "        break\n",
        "     print 'User ' + str(word) + ' has a total play count of ' + str(count) + ' and a mean play count of ' + str(count/user_count_number[word]) \n",
        "     top += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9PhJkBdujvV"
      },
      "source": [
        "####  Splitting Data for Testing\n",
        "\n",
        "Used the [randomSplit](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.randomSplit) function to divide the data (`userArtistData`) into:\n",
        "* A training set, `trainData`, that will be used to train the model. This set should constitute 40% of the data.\n",
        "* A validation set, `validationData`, used to perform parameter tuning. This set should constitute 40% of the data.\n",
        "* A test set, `testData`, used for a final evaluation of the model. This set should constitute 20% of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnVSFupv-7yz",
        "outputId": "22303277-d2da-4526-abb3-ae6a08f2920f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(1059637, 1000049, 1), (1059637, 1000056, 1), (1059637, 1000113, 5)]\n",
            "[(1059637, 1000010, 238), (1059637, 1000062, 11), (1059637, 1000112, 423)]\n",
            "[(1059637, 1000094, 1), (1059637, 1000130, 19129), (1059637, 1000139, 4)]\n",
            "19817\n",
            "19633\n",
            "10031\n"
          ]
        }
      ],
      "source": [
        "#Splitting the data into train, test and cross validation\n",
        "trainData, validationData, testData = userArtistData.randomSplit([4, 4, 2], 13)\n",
        "\n",
        "print trainData.take(3)\n",
        "print validationData.take(3)\n",
        "print testData.take(3)\n",
        "print trainData.count()\n",
        "print validationData.count()\n",
        "print testData.count()\n",
        "\n",
        "#Caching and creating ratings object\n",
        "trainData = trainData.map(lambda l: Rating(*l)).cache()\n",
        "validationData = validationData.map(lambda l: Rating(*l)).cache()\n",
        "testData = testData.map(lambda l: Rating(*l)).cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84BGOnQPujvX"
      },
      "source": [
        "## The Recommender Model\n",
        "\n",
        "For this project, I will train the model with implicit feedback. \n",
        "\n",
        "You can read more information about this from the collaborative filtering page: [http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html](http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html). \n",
        "\n",
        "The [function i have used here](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.recommendation.ALS.trainImplicit) has a few tunable parameters that will affect how the model is built. \n",
        "\n",
        "Therefore, to get the best model, we will do a small parameter sweep and choose the model that performs the best on the validation set\n",
        "\n",
        "Therefore, we must first devise a way to evaluate models. Once we have a method for evaluation, we can run a parameter sweep, evaluate each combination of parameters on the validation data, and choose the optimal set of parameters. The parameters then can be used to make predictions on the test data.\n",
        "\n",
        "### Model Evaluation\n",
        "\n",
        "Although there may be several ways to evaluate a model, I will use a simple method here. Suppose we have a model and some dataset of *true* artist plays for a set of users. This model can be used to predict the top X artist recommendations for a user and these recommendations can be compared the artists that the user actually listened to (here, X will be the number of artists in the dataset of *true* artist plays). Then, the fraction of overlap between the top X predictions of the model and the X artists that the user actually listened to can be calculated. This process can be repeated for all users and an average value returned.\n",
        "\n",
        "For example, suppose a model predicted [1,2,4,8] as the top X=4 artists for a user. Suppose, that user actually listened to the artists [1,3,7,8]. Then, for this user, the model would have a score of 2/4=0.5. To get the overall score, this would be performed for all users, with the average returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxBEdMDX-7y1"
      },
      "outputs": [],
      "source": [
        "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
        "from collections import defaultdict\n",
        "\n",
        "#model evaluation function\n",
        "def modelEval(model, dataset):\n",
        "    global trainData\n",
        "    global allArtists\n",
        "    \n",
        "    #Getting nonTrainArtists for each user\n",
        "    userArtists = defaultdict(list)\n",
        "    \n",
        "    for data in trainData.collect():\n",
        "        userArtists[data[0]].append(data[1])\n",
        "        \n",
        "    cvList = []\n",
        "        \n",
        "    for key in userArtists.keys():\n",
        "        userArtists[key] = list(set(allArtists) - set(userArtists[key]))\n",
        "        for artist in userArtists[key]:\n",
        "            cvList.append((key, artist))\n",
        "    \n",
        "    #Creating user,nonTrainArtists RDD\n",
        "    cvData = sc.parallelize(cvList)\n",
        "    \n",
        "    userOriginal = dataset.map(lambda x:(x.user, (x.product, x.rating))).groupByKey().collect()\n",
        "    \n",
        "    #prediction on the user, nonTrainArtists RDD\n",
        "    predictions = model.predictAll(cvData)\n",
        "    userPredictions = predictions.map(lambda x:(x.user, (x.product, x.rating))).groupByKey().collect()\n",
        "    original = {}\n",
        "    predictions = {}\n",
        "    \n",
        "    #Getting top X artists for each user\n",
        "    for line in userOriginal:\n",
        "        original[line[0]] = sorted(line[1], key=lambda x:x[1], reverse = True)\n",
        "        \n",
        "    for line in userPredictions:\n",
        "        predictions[line[0]] = sorted(line[1], key=lambda x:x[1], reverse = True)\n",
        "        \n",
        "    similarity = []\n",
        "    \n",
        "    for key in userOriginal:\n",
        "        similar = 0.0\n",
        "        \n",
        "        pred = predictions[key[0]]\n",
        "        org = original[key[0]]\n",
        "            \n",
        "        for value in org:\n",
        "            for item in pred[0:len(org)]:\n",
        "                if (value[0] == item[0]):\n",
        "                    similar += 1\n",
        "                    break\n",
        "                    \n",
        "        #Similarity calculation        \n",
        "        similarity.append(float(similar/len(org)))        \n",
        "        \n",
        "    string = \"The model score for rank \" + str(rank) + \" is \" + str(float(sum(similarity)/len(similarity)))    \n",
        "    print string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNCKBK5rujvZ"
      },
      "source": [
        "### Model Construction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTHNA-f_-7y3",
        "outputId": "384129d2-45aa-475c-833f-17f7525b3e48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model score for rank 2 is 0.0909391661474\n",
            "The model score for rank 10 is 0.0957125879247\n",
            "The model score for rank 20 is 0.09047041725\n"
          ]
        }
      ],
      "source": [
        "#Model evaluation through different rank parameters\n",
        "rank_list = [2, 10, 20]\n",
        "\n",
        "for rank in rank_list:\n",
        "    model = ALS.trainImplicit(trainData, rank, seed=345)\n",
        "    modelEval(model,validationData)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFRZQoGB-7y4",
        "outputId": "4ed86bd6-67f6-409a-d49f-bc94a36f380a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model score for rank 20 is 0.0512321818394\n"
          ]
        }
      ],
      "source": [
        "bestModel = ALS.trainImplicit(trainData, rank=10, seed=345)\n",
        "modelEval(bestModel, testData)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "EUDY6IgLAaec"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjVDxHvUujvc"
      },
      "source": [
        "## Trying Some Artist Recommendations\n",
        "Using the best model above, I predicted the top 5 artists for user `1059637` using the [recommendProducts](http://spark.apache.org/docs/1.5.2/api/python/pyspark.mllib.html#pyspark.mllib.recommendation.MatrixFactorizationModel.recommendProducts) function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O68e0B0D-7y5"
      },
      "outputs": [],
      "source": [
        "ratings = bestModel.recommendProducts(1059637, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwrHIyCm-7y6",
        "outputId": "760639b0-3291-485a-c907-8e1c46d6e641"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Artist 0: Brand New\n",
            "Artist 1: Taking Back Sunday\n",
            "Artist 2: Evanescence\n",
            "Artist 3: Elliott Smith\n",
            "Artist 4: blink-182\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "artist_data = artistData.collect()\n",
        "\n",
        "artist_names_dict = {}\n",
        "\n",
        "for line in artist_data:\n",
        "    pattern = re.match( r'(\\d+)(\\s+)(.*)', line)\n",
        "    artist_names_dict[str(pattern.group(1))] = pattern.group(3)\n",
        "\n",
        "for i in range(0,5):\n",
        "    if str(ratings[i].product) in artist_canonical_dict:\n",
        "        artist_id = artist_canonical_dict[str(ratings[i].product)]\n",
        "        print \"Artist \" + str(i) + \": \"  + str(artist_names_dict[str(artist_id)])\n",
        "    else:\n",
        "        print \"Artist \" + str(i) + \": \" + str(artist_names_dict[str(ratings[i].product)])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.9"
    },
    "colab": {
      "name": "AU1940209.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
